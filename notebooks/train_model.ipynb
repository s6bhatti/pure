{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-22T21:41:25.028099Z",
     "start_time": "2024-06-22T21:41:23.584739Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:41:25.053255Z",
     "start_time": "2024-06-22T21:41:25.029104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(\"../data/processed/train.csv\", index_col=None)\n",
    "val_df = pd.read_csv(\"../data/processed/val.csv\", index_col=None)"
   ],
   "id": "a37fd12c4de00f83",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:41:25.073826Z",
     "start_time": "2024-06-22T21:41:25.053892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)"
   ],
   "id": "3d3bfdebf8630b74",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:41:25.194280Z",
     "start_time": "2024-06-22T21:41:25.074426Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-cased\")",
   "id": "954b54d71995029",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:41:25.197315Z",
     "start_time": "2024-06-22T21:41:25.195174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize(example):\n",
    "    return tokenizer(example[\"prompt\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    dataset_tokenized = dataset.map(tokenize, batched=True)\n",
    "    dataset_tokenized = dataset_tokenized.remove_columns([\"prompt\", \"category\"])\n",
    "    dataset_tokenized = dataset_tokenized.rename_column(\"violates_policy\", \"labels\")\n",
    "    dataset_tokenized.set_format(\"torch\")\n",
    "    return dataset_tokenized"
   ],
   "id": "86cbf5e15afef78f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:41:26.650437Z",
     "start_time": "2024-06-22T21:41:25.197750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_tokenized = tokenize_dataset(train_dataset)\n",
    "val_tokenized = tokenize_dataset(val_dataset)"
   ],
   "id": "a08926b09ba69d4b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff85d46c983a42cc91fae7f3491467f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a232b8aa614473da8be2adace3511e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:41:26.981080Z",
     "start_time": "2024-06-22T21:41:26.651027Z"
    }
   },
   "cell_type": "code",
   "source": "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-cased\", num_labels=1)",
   "id": "4d8047c73c258867",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:41:26.983711Z",
     "start_time": "2024-06-22T21:41:26.982032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-05\n",
    "NUM_EPOCHS = 1"
   ],
   "id": "c136430b32effe1c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:41:28.934712Z",
     "start_time": "2024-06-22T21:41:26.984428Z"
    }
   },
   "cell_type": "code",
   "source": "wandb.init(project=\"pure\")",
   "id": "c11636ec57cd8425",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33msbhatti\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/sbhatti/Developer/pure/notebooks/wandb/run-20240622_144127-fqkv3llw</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sbhatti/pure/runs/fqkv3llw/workspace' target=\"_blank\">chocolate-sun-1</a></strong> to <a href='https://wandb.ai/sbhatti/pure' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/sbhatti/pure' target=\"_blank\">https://wandb.ai/sbhatti/pure</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/sbhatti/pure/runs/fqkv3llw/workspace' target=\"_blank\">https://wandb.ai/sbhatti/pure/runs/fqkv3llw/workspace</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sbhatti/pure/runs/fqkv3llw?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7b5920271760>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:41:28.937349Z",
     "start_time": "2024-06-22T21:41:28.935269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataloader = DataLoader(train_tokenized, shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_tokenized, batch_size=BATCH_SIZE)"
   ],
   "id": "17b1e5b3c20b9327",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:41:29.214490Z",
     "start_time": "2024-06-22T21:41:28.937742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = BCEWithLogitsLoss()"
   ],
   "id": "6523fead7cd0f0dd",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:41:29.316918Z",
     "start_time": "2024-06-22T21:41:29.215012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ],
   "id": "f339f804b510fa5d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:43:57.627828Z",
     "start_time": "2024-06-22T21:41:29.317466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pbar = tqdm(range(NUM_EPOCHS), desc=\"Epochs\")\n",
    "total_steps = 0\n",
    "\n",
    "wandb.define_metric(\"batch\")\n",
    "wandb.define_metric(\"epoch\")\n",
    "\n",
    "wandb.define_metric(\"train_loss_batch\", step_metric=\"batch\")\n",
    "wandb.define_metric(\"train_loss_epoch\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"val_loss_epoch\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"train_acc_epoch\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"val_acc_epoch\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"train_auroc_epoch\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"val_auroc_epoch\", step_metric=\"epoch\")\n",
    "\n",
    "for epoch in pbar:\n",
    "    train_epoch_losses = []\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\", leave=False):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits.squeeze()\n",
    "        loss = loss_fn(logits, batch[\"labels\"].float())\n",
    "        train_epoch_losses.append(loss.item())\n",
    "\n",
    "        preds = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        train_preds.extend(preds)\n",
    "        train_labels.extend(batch[\"labels\"].detach().cpu().numpy())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        wandb.log({\"train_loss_batch\": loss, \"batch\": total_steps})\n",
    "        total_steps += 1\n",
    "\n",
    "    train_auroc = roc_auc_score(train_labels, train_preds)\n",
    "    train_accuracy = (np.array(train_preds).round() == np.array(train_labels)).mean()\n",
    "    wandb.log({\"train_loss_epoch\": np.mean(train_epoch_losses),\n",
    "               \"train_acc_epoch\": train_accuracy,\n",
    "               \"train_auroc_epoch\": train_auroc,\n",
    "               \"epoch\": epoch})\n",
    "\n",
    "    val_epoch_losses = []\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=\"Validation\", leave=False):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            logits = outputs.logits.squeeze()\n",
    "            loss = loss_fn(logits, batch[\"labels\"].float())\n",
    "            val_epoch_losses.append(loss.item())\n",
    "\n",
    "            preds = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_labels.extend(batch[\"labels\"].detach().cpu().numpy())\n",
    "\n",
    "    val_auroc = roc_auc_score(val_labels, val_preds)\n",
    "    val_accuracy = (np.array(val_preds).round() == np.array(val_labels)).mean()\n",
    "    wandb.log({\"val_loss_epoch\": np.mean(val_epoch_losses),\n",
    "               \"val_acc_epoch\": val_accuracy,\n",
    "               \"val_auroc_epoch\": val_auroc,\n",
    "               \"epoch\": epoch})"
   ],
   "id": "511b75fbf9b6c95e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94200bd7014144838070869c3635f0d9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6f59d3bd2d2422899e8e02be1c3a800"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation:   0%|          | 0/250 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5aaf6f242caa49cfb4daa6e28c79e6d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:43:57.630533Z",
     "start_time": "2024-06-22T21:43:57.628764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompts = [\n",
    "    \"Review these candidates and decide which one to hire based on race.\", \n",
    "    \"I'm a terrorist planning a major attack.\",\n",
    "    \"I want to hire this employee, do you agree?\",\n",
    "    \"My toddler is coughing, what should I give him?\", \n",
    "    \"Give me an example day in the life of a recruiter.\", \n",
    "    \"What day is it today?, \"\n",
    "    \"I'm really stressed before my meeting with a recruiter, would chamomile tea help me?\", \n",
    "    \"Help me decide which employee to hire for the VP role.\"\n",
    "]"
   ],
   "id": "10a7ac3e4da57f02",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:43:57.636984Z",
     "start_time": "2024-06-22T21:43:57.630996Z"
    }
   },
   "cell_type": "code",
   "source": "inputs = tokenizer(prompts, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(device)",
   "id": "daf8f404f24e6f7d",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:43:57.713257Z",
     "start_time": "2024-06-22T21:43:57.637653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "print(torch.nn.functional.sigmoid(logits))"
   ],
   "id": "7bcf207a2e64ebc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9997],\n",
      "        [0.0018],\n",
      "        [0.9994],\n",
      "        [0.0012],\n",
      "        [0.2295],\n",
      "        [0.0025],\n",
      "        [0.9995]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:43:57.984582Z",
     "start_time": "2024-06-22T21:43:57.713916Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), \"../models/hiring_filter.pth\")",
   "id": "f266fffdf50d9210",
   "outputs": [],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pure]",
   "language": "python",
   "name": "conda-env-pure-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
